# 概述

> 对抗样本：对输入进行微小的偏差，但是刻意与数据集的其他样本不同（加上误分类的扰动）
>
> 导致输出高可信度的错误分类结果
>
> 产生的原因：神经网络的线性特征
>
> 在结构上和训练集上对对抗样本进行正则化
>
> 多个不同结构的，使用不同训练集的神经网络，可能对同一个对抗样本做出同样的判断，说明了训练模型算法的缺陷
>
> 高维空间的线性特征足以产生对抗样本，也提供了一种用来训练模型，获取对抗样本的方法。
>
> 通过对抗样本训练，可以得到额外的正则化收益
>
> 常用的防止过拟合算法，例如Dropout，pretraining，model average无法提高模型对对抗样本的抵抗性，但转换成非线性模型可以。

> Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征
>
> ![image-20220925214326181](C:\Users\8208191402\AppData\Roaming\Typora\typora-user-images\image-20220925214326181.png)

> model average，涉及到federal training。Federated Averaging 算法，终端用户本地进行SGD训练，然后服务器进行一个模型求平均过程。
>
> 每个用户（Client）有一个本地训练数据集，这个数据集不上传到服务器。然后每个用户从服务器获取参数并且通过某种方法使得全局模型可以随着本地训练进行更新。
>
> 对比传统中心化模式，数据需要被传输到中心节点，在FL下，只需要传递参数就可以了