@[TOC](用对抗样本抵御对抗样本)

# Introduction

> 对抗样本：被人为修改的，添加了不可察觉的（差值小于给定值）误差噪声的样本，用于误导网络模型做出高置信度的错误分类。研究表明，对抗样本的出现是高维数据自然分布（high-dimensional natural data distribution）的固有特点。

> 目前的作法之一是隐藏（conceal）网络关于输入图像的梯度（因为梯度对于产生对抗样本至关重要），但收效甚微（Athalye et al）；我们的做法是：在输入之前，通过在一个外部模型上训练得到的对抗样本来转换输入格式，以提高模型的鲁棒性，并减小了训练代价。

$$
f:classification\\
g(.):the \quad  process \quad of  \quad finding \quad an \quad adversial \quad example \quad near \quad \pmb{x}\\
f(x) -> f(g(x))
$$

> 模型的鲁棒性来自于估算g(x)对于x的梯度，寻找对抗样本等价于在一个高度凹函数上寻找局部最小值点。
>
> 因此，g(x)是一个迭代函数，通过随机初始化参数和不可为操作实现局部最小点的寻找。

# 相关工作

- 对抗攻击类别
  - Fast Gradient Sign Method（FGSM）
  - 白盒攻击
    - Projected Gradient Descent（PGD），是基于模型一阶信息（关于输入的梯度）的，最强的攻击方式，但是必须要知晓模型的网络结构和参数，也就是白盒攻击，本模型是基于白盒攻击背景下的。
    - 通过对输入图像进行随机转换或使用随机的激活函数的方法，以及使用不可微操作，但对于对抗样本不是很有效。
    - 对抗白盒攻击最好的办法是对抗性训练。
    - Backward Pass Differentiable Approximation，通过对梯度进行近似的方法来避开梯度混淆方法。
  - 黑盒攻击
    - transfer attack，使用一个模型产生的对抗样本来攻击另一个模型，强调可移植性
    - query attack，用不同输入，观察模型的行为，构建对抗样本。
  - 